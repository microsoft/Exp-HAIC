<!DOCTYPE html>
<html lang="en">

<head>
    {% load static %}
    <title>Instructions</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous">
    </script>
    <link rel="stylesheet" type="text/css" href="{% static 'myapp/instructionformat.css' %}" />

</head>

<body>

    <div class="jumbotron text-center">
        <h3>Additional AI information:<b> {% if exp_aism == 1 %} AI focus {% elif exp_aieof == 1 %} explanations of
                failure {% endif %}
                {% if exp_stage == 3 or exp_stage == 5 or exp_stage == 7 %} <span style="color:red">(new!)</span>
                {% endif %}</b></h3>
    </div>

    <div class="container" style='width:60%'>
        <div class="row">
            <p>Until now, you have been shown the probability predictions and the recommendations generated by the AI
                diagnostic tool.
            </p>
            <p>In this stage of the experiment, you will have access to additional information regarding the AI tool's
                predictions,
                {% if exp_aism == 1 %}
                which we call the <span style="font-style: italic;"> AI focus. </span> The AI focus highlights the
                regions of the image that the tool "looks" at while generating a certain prediction. The regions colored
                in red are those that have the highest impact on the predicition made by the AI diagnostic tool. Those
                colored in blue, instead, have the smallest impact
                on the AI's predictions. For example, the AI focus might highlight the patient's heart or their veins
                in case of heart-related diseases. In the machine learning community, the AI focus is known as "saliency
                map". The saliency maps that will be shown to you have been generated using a technique called
                "GradCAM". The figure below shows an example of AI focus.
                <div class="imageWrapper"><img class="gif"
                        src={% static 'myapp\images\interface-images\gif_aifocus.gif' %} /></div>
                Once you have visualized the AI focus, you can let us know whether it was useful or not.
                See the example below.
                <div class="imageWrapper" style="width:75%;margin:0 auto;display:block;"><img class="gif"
                        src={% static 'myapp\images\interface-images\gif_aifocususeful.gif' %} /></div>
            </p>
            {% elif exp_aieof == 1 %}
            the <i> global explanations of failure.</i> As mentioned in past instructions, the probability
            estimates generated by the AI tool for the presence of a condition may not be well calibrated. This
            means that, for example, among all the images for which the tool predicts that some condition is
            present with probability 80%, the condition might actually be present only 60% of the times.
            Explanations of failure represent a measure of the accuracy of the tool's predictions that overcomes
            this limitation. For each X-ray, global explanations of failure for a certain condition correspond to the
            accuracy of the AI tool measured on images for which the tool has made
            the same (binary) prediction. For example, in the case of (say) cardiomegaly, an explanation of failure
            could be "Overall, the AI's accuracy on cases where it predicts that
            this condition [cardiomegaly] is present is 70%." If the AI diagnostic tool instead predicts that the
            condition is not present, another explanation of failure may be "Overall,
            the AI's accuracy on cases where it predicts that this condition [cardiomegaly] is not present is 95%." See
            two examples below.
            </p>
            <div class="imageWrapper" style="width:75%;margin:0 auto;display:block;"><img class="gif"
                    src={% static 'myapp\images\interface-images\ai_eofex1.png' %} />
            </div>
            <div class="imageWrapper" style="width:75%;margin:0 auto;display:block;"><img class="gif"
                    src={% static 'myapp\images\interface-images\ai_eofex2.png' %} />
            </div>


            <h5>How do the AI's estimated probabilities and explanations of failure differ?</h5>


            <p style="text-align: justify">Note that explanations of failure and AI's
                estimated probabilities are very different concepts.
                <ul>
                    <li> Explanation of failure indicates the accuracy of the model for a certain condition,
                        given a certain (binary) prediction made by the AI tool. The accuracy values are computed
                        based on past model behavior for that specific condition. Technically,
                        explanations of failure are known as <u>positive predicted values</u> and
                        <u>negative predicted values.</u> Note that these measures are different from true
                        positive rates (a.k.a. sensitivity=1-false negative rate) and true negative rates
                        (a.k.a. specificity, =1-false positive rate) that you are likely used to dealing
                        with. In essence, explanations of failure and true negative/positive rates
                        differ in terms of the event on which we are conditioning. Since we cannot know
                        ahead of time whether the condition is present or not, global explanations of
                        failure condition on the binary model prediction.</li>
                    <li> The AI's estimated probability, instead, is the probability that the condition is present,
                        according to the AI tool. This may not always correspond to the real accuracy of
                        the tool. Clearly, we would hope that the AI tool will be very accurate when its
                        estimated probability that the condition is present is either low or high. Due
                        to potential lack of calibration, not all AI tools enjoy this property.
                        Sometimes, high estimated probabilities for a condition may still correspond to
                        the condition not being present, and vice versa. This is the reason why one may
                        also need to rely on explanations of failure, which provide additional insights
                        into the accuracy of the tool's predictions for the condition.</li>
                </ul>
            </p>
        </div>
        {% else %}
        ...no additional AI information is shown in this stage other than confidence! And you should not be seeing this
        message. </p>
        {% endif %}
        <form method="POST">
            {% csrf_token %}
            <button class="btn btn-primary" value='previous'>&laquo; Previous</button>
            <button class="btn btn-primary float-end" value='next' name='next'>Next &raquo;</button>
        </form>
    </div>
</body>

</html>